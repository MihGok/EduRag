# Dockerfile
FROM nvidia/cuda:12.9.0-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
SHELL ["/bin/bash", "-c"]

# Системные пакеты для сборки
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential git curl ca-certificates cmake pkg-config \
    python3 python3-dev python3-venv python3-distutils python3-pip \
    libsndfile1 && rm -rf /var/lib/apt/lists/*

# Создаём виртуальное окружение
ENV VENV_PATH=/opt/venv
RUN python3 -m venv ${VENV_PATH}
ENV PATH="${VENV_PATH}/bin:${PATH}"

# Обновим pip/setuptools/wheel и cmake
RUN python -m pip install --upgrade pip setuptools wheel
RUN python -m pip install --no-cache-dir --upgrade cmake

# Принудительно пересобираем llama-cpp-python с поддержкой CUDA (GGML_CUDA=ON)
RUN python -m pip uninstall -y llama-cpp-python || true
RUN python -m pip cache purge || true
# Устанавливаем из исходников с передачей CMake-аргументов
RUN CMAKE_ARGS="-DGGML_CUDA=ON" \
    python -m pip install --no-cache-dir --force-reinstall \
      llama-cpp-python --config-setting="cmake.args=-DGGML_CUDA=ON"

# Устанавливаем FastAPI/uvicorn и вспомогательные библиотеки
RUN python -m pip install --no-cache-dir fastapi "uvicorn[standard]" pydantic

# Рабочая директория для endpoint'а
WORKDIR /srv/endpoint
# Копируем только endpoint (ожидается, что у тебя есть app/main.py локально)
COPY app/main.py /srv/endpoint/main.py

EXPOSE 8000

# По умолчанию — запускаем uvicorn из venv. Этот CMD можно переопределить в docker-compose.
CMD ["/opt/venv/bin/uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
